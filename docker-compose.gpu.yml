version: '3.8'

services:
  hr-assessment-gpu:
    build: .
    container_name: hr-assessment-pipeline-gpu
    volumes:
      # Mount your audio data (add more volumes as needed)
      - ./audio:/app/audio:ro
      # Mount output directory
      - ./outputs:/app/outputs
      # Mount .env file
      - ./.env:/app/.env:ro
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GROQ_MODEL=${GROQ_MODEL:-llama-3.3-70b-versatile}
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
      - WHISPER_DEVICE=cuda
      - EMOTION_DEVICE=cuda
      - TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6"
    # Allocate more memory for heavy models
    mem_limit: 16g
    shm_size: 4g
    # Use all available CPUs
    cpus: "8.0"
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Keep container running for interactive use
    stdin_open: true
    tty: true
    # Override default command
    command: tail -f /dev/null

  # Optional: API service with GPU
  hr-assessment-api-gpu:
    build: .
    container_name: hr-assessment-api-gpu
    ports:
      - "8000:8000"
    volumes:
      - ./outputs:/app/outputs
      - ./.env:/app/.env:ro
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - WHISPER_DEVICE=cuda
      - EMOTION_DEVICE=cuda
      - TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6"
    mem_limit: 16g
    shm_size: 4g
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: uvicorn api:app --host 0.0.0.0 --port 8000
