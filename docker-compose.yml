services:
  hr-assessment:
    build: .
    container_name: hr-assessment-pipeline
    volumes:
      # Mount Team Recordings folder
      - ./Team Recordings:/app/Team Recordings:ro
      # Mount output directory
      - ./outputs:/app/outputs
      # Mount .env file (optional)
      - ./.env:/app/.env:ro
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GROQ_MODEL=${GROQ_MODEL:-llama-3.3-70b-versatile}
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
      - WHISPER_DEVICE=${WHISPER_DEVICE:-cuda}
      - EMOTION_DEVICE=${EMOTION_DEVICE:-cuda}
      - CUDA_VISIBLE_DEVICES=1
      - TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6"
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/targets/x86_64-linux/lib:${LD_LIBRARY_PATH}
    # Allocate memory for models
    shm_size: 4g
    # Use available CPUs
    cpus: "8.0"
    # GPU support - use GPU 1 specifically
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    # Keep container running for interactive use
    stdin_open: true
    tty: true
    # Override default command
    command: tail -f /dev/null

  # Optional: API service
  hr-assessment-api:
    build: .
    container_name: hr-assessment-api
    ports:
      - "8000:8000"
    volumes:
      - ./outputs:/app/outputs
      - ./.env:/app/.env:ro
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - WHISPER_DEVICE=${WHISPER_DEVICE:-cuda}
      - TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6"
    mem_limit: 8g
    shm_size: 2g
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: uvicorn api:app --host 0.0.0.0 --port 8000
