# HR Personality & Motivation Assessment Pipeline

AI-powered voice analysis pipeline that extracts **Big Five personality traits** and **motivation/engagement levels** from audio recordings using voice features (prosody, emotion, acoustics) and optional transcripts.

## ğŸ¯ Key Features

- **Voice-Only Analysis** â€” Assess personality and motivation from voice features alone (no transcript needed)
- **Stable Scoring** â€” Deterministic formulas ensure consistent results for the same speaker
- **Universal Audio Input** â€” Single file or folder, any audio format (WAV, MP3, M4A, AAC, FLAC, OGG)
- **GPU Acceleration** â€” CUDA support with automatic GPU selection for shared servers
- **Prosody Analysis** â€” Speaking rate, pitch variance, energy, pauses, rhythm
- **Emotion Detection** â€” emotion2vec model with CPU fallback
- **Acoustic Features** â€” eGeMAPS via OpenSMILE
- **AI Assessment** â€” Groq LLM (LLaMA 3.3 70B) with deterministic scoring (temperature=0.0)
- **Multiple Outputs** â€” JSON reports, HTML reports, batch summary tables
- **Beautiful Visualization** â€” Color-coded progress bars for motivation and engagement

## ğŸ“Š Assessment Output

### Big Five Personality Profile (0â€“100)

| Trait | What it measures | Voice indicators |
|-------|-----------------|------------------|
| **Openness** | Creativity, curiosity | Wide pitch range, expressive tone |
| **Conscientiousness** | Organization, discipline | Steady pace, few fillers, structured speech |
| **Extraversion** | Sociability, assertiveness | Fast rate, high energy, loud volume |
| **Agreeableness** | Cooperation, trust | Warm prosody, smooth pitch |
| **Neuroticism** | Emotional instability | Unstable pitch, many pauses, rough voice |

### Motivation & Engagement Assessment

- **Motivation Score** (0-100) â€” Computed from voice features using deterministic formulas
- **Engagement Score** (0-100) â€” Derived from motivation + extraversion
- **Level** â€” High / Medium / Low (with hysteresis to prevent flickering)
- **Pattern** â€” Rising / Falling / Consistent / Fluctuating
- **Voice Indicators** â€” Energy, speaking rate, pauses, pitch dynamics, emotion

**Example Output:**
```
Motivation & Engagement Analysis:
  Overall Motivation [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Low (20/100)
  Pattern: Consistent

  Voice-Based Indicators:
    â€¢ energy_mean=0.028 (low)
    â€¢ speaking_rate_wpm=104 (slow)
    â€¢ pauses_per_minute=7.2 (high)
    â€¢ pitch_variance=250 (low)

  Engagement Level   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Low (30/100)
  Derived from motivation (20) and extraversion (30)
```

## ğŸš€ Quick Start

### Installation

```bash
cd SA_emotion_detection
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

Configure API key:
```bash
cp .env.example .env
# Edit .env â†’ set GROQ_API_KEY
```

### Basic Usage

```bash
# Single audio file (voice-only analysis)
python main.py interview.wav --skip-transcription

# Folder of audio files
python main.py recordings/ --skip-transcription --limit 5

# With transcript
python main.py interview.wav --transcript interview.txt

# Generate HTML report
python main.py interview.wav --html-report
```

## ğŸ³ Docker Deployment

### Quick Start (Automatic GPU Selection)

```bash
# Build Docker image
docker compose build

# Check available GPUs
./check_gpu.sh

# Run pipeline (automatically selects free GPU)
./run_pipeline.sh "Team Recordings/Digvijay/Audio/" --skip-transcription --limit 2
```

### Manual Docker Commands

```bash
# Build
docker compose build hr-assessment

# Run on specific GPU (e.g., GPU 1)
docker run -d --name hr-assessment-pipeline \
  --gpus '"device=1"' \
  -v "$(pwd)/Team Recordings:/app/Team Recordings:ro" \
  -v "$(pwd)/outputs:/app/outputs" \
  -v "$(pwd)/.env:/app/.env:ro" \
  -e GROQ_API_KEY \
  -e CUDA_VISIBLE_DEVICES=1 \
  -e WHISPER_DEVICE=cuda \
  -e EMOTION_DEVICE=cuda \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  --shm-size=8g \
  sa_emotion_detection-hr-assessment tail -f /dev/null

# Run pipeline inside container
docker exec hr-assessment-pipeline python main.py "Team Recordings/Digvijay/Audio/" --skip-transcription

# Stop container
docker stop hr-assessment-pipeline && docker rm hr-assessment-pipeline
```

## ğŸ–¥ï¸ GPU Usage (Shared Server)

### Automatic GPU Selection

The `run_pipeline.sh` script automatically finds a free GPU and starts the container:

```bash
# Check available GPUs
./check_gpu.sh

# Run pipeline (auto-selects free GPU)
./run_pipeline.sh "Team Recordings/Digvijay/Audio/" --skip-transcription
```

**Example output:**
```
ğŸ” Searching for available GPU...
âœ… Found free GPU: 2 (Quadro RTX 6000, 23.5 GB free)
ğŸš€ Starting container on GPU 2...
âœ… Container started successfully on GPU 2
```

### Using Multiple GPUs

Distribute models across 2 GPUs for better performance:

```bash
docker run -d --name hr-assessment-pipeline \
  --gpus '"device=0,1"' \
  -v "$(pwd)/Team Recordings:/app/Team Recordings:ro" \
  -v "$(pwd)/outputs:/app/outputs" \
  -e GROQ_API_KEY \
  -e CUDA_VISIBLE_DEVICES=0,1 \
  -e WHISPER_DEVICE=cuda:0 \
  -e EMOTION_DEVICE=cuda:1 \
  --shm-size=8g \
  sa_emotion_detection-hr-assessment tail -f /dev/null
```

**Model distribution:**
- **GPU 0**: Whisper, WavLM, prosody
- **GPU 1**: emotion2vec

### Best Practices for Shared Servers

1. **Always check GPU availability** before running: `./check_gpu.sh`
2. **Use automatic GPU selection**: `./run_pipeline.sh`
3. **Stop container when done**: `docker stop hr-assessment-pipeline`
4. **Monitor GPU usage**: `watch -n 1 nvidia-smi`
5. **Be considerate** â€” don't occupy GPUs unnecessarily

## ğŸ“ CLI Options

```bash
python main.py <input_path> [options]

Positional:
  input_path              Audio file or folder

Options:
  -t, --transcript PATH   Transcript file (.txt/.json)
  -c, --candidate-id ID   Candidate identifier
  -p, --position ROLE     Position / role context
  -o, --output-dir DIR    Output directory (default: ./outputs)
  -l, --limit N           Max files to process
  --skip-transcription    Voice-only analysis (no Whisper)
  --whisper-model SIZE    tiny/base/small/medium/large (default: base)
  --html-report           Generate HTML report per file
  --group-by-folder       Group results by parent folder
  --no-save               Don't write JSON output
  -q, --quiet             Suppress detailed output
```

## ğŸ”¬ How It Works

### 1. Voice Feature Extraction

**Prosody Features:**
- Speaking rate (words per minute)
- Pitch variance, range, mean, slope
- Energy mean, std, range
- Pauses per minute, long pauses count
- Speech-to-silence ratio
- Rhythm regularity

**Emotion Detection:**
- emotion2vec model (9 emotions: happy, sad, angry, neutral, fearful, surprised, disgusted, contempt, unknown)
- Primary emotion + confidence score
- Emotion timeline

**Acoustic Features:**
- eGeMAPS (88 features via OpenSMILE)
- Voice quality (HNR, jitter, shimmer)
- Spectral features

### 2. Deterministic Scoring

**Motivation Score (0-100):**
```
Start: 50

Energy:     if energy_mean >= 0.06: +15, if <= 0.03: -15
Pace:       if speaking_rate >= 150: +15, if <= 110: -15
Pauses:     if pauses_per_minute <= 3: +10, if >= 6: -10
Pitch:      if pitch_variance >= 800: +10, if <= 300: -10
Emotion:    if happy/surprised + conf >= 0.5: +10
            if sad/fearful + conf >= 0.5: -10

Clamp to [0, 100]
```

**Engagement Score (0-100):**
```
engagement_score = round(0.6 * motivation_score + 0.4 * extraversion_score)
```

**Hysteresis (Stable Levels):**
- Scores within Â±7 points of boundaries (40, 70) â†’ set to "Medium" to prevent flickering
- Ensures same speaker gets consistent level across multiple recordings

### 3. LLM Assessment

- **Model**: Groq LLaMA 3.3 70B
- **Temperature**: 0.0 (deterministic output)
- **Input**: Voice features + optional transcript
- **Output**: Big Five scores, motivation/engagement, strengths, development areas, HR summary

## ğŸ“‚ Output Files

### JSON Report
```json
{
  "metadata": {
    "audio_file": "interview.wav",
    "candidate_id": "C001",
    "timestamp": "20260215_120000"
  },
  "assessment": {
    "big_five": {
      "openness": {"score": 65, "confidence": 80, "reason": "..."},
      "conscientiousness": {"score": 72, "confidence": 85, "reason": "..."},
      ...
    },
    "motivation": {
      "overall_level": "High",
      "motivation_score": 75,
      "pattern": "Rising",
      "voice_indicators": ["high energy", "fast pace", ...]
    },
    "engagement": {
      "overall_level": "High",
      "engagement_score": 78,
      "reason": "Derived from motivation (75) and extraversion (85)"
    },
    "trait_strengths": ["Conscientiousness", "Extraversion", ...],
    "hr_summary": "..."
  }
}
```

### Console Output
```
============================================================
HR ASSESSMENT SUMMARY
============================================================
Candidate: John_Doe

Big Five Personality Profile:
  Openness           [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘] 65/100 (80% conf)
  Conscientiousness  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 72/100 (85% conf)
  Extraversion       [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 85/100 (90% conf)
  Agreeableness      [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 60/100 (75% conf)
  Neuroticism        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 30/100 (70% conf)

Motivation & Engagement Analysis:
  Overall Motivation [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘] High (75/100)
  Pattern: Rising

  Voice-Based Indicators:
    â€¢ energy_mean=0.065 (high)
    â€¢ speaking_rate_wpm=165 (fast)
    â€¢ pauses_per_minute=2.5 (low)
    â€¢ pitch_variance=850 (high)

  Engagement Level   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘] High (78/100)
  Derived from motivation (75) and extraversion (85)

Key Strengths:
  â€¢ Extraversion
  â€¢ Conscientiousness
  â€¢ Achievement-Striving

Development Areas:
  â€¢ Openness
  â€¢ Agreeableness

âœ“ Processing completed in 125.34s
============================================================
```

## ğŸ Python API

```python
from src.pipeline import HRAssessmentPipeline
from src.config import load_config

# Initialize pipeline
pipeline = HRAssessmentPipeline(load_config())

# Process audio (voice-only)
result = pipeline.process(
    audio_path="interview.wav",
    candidate_id="C001",
    skip_transcription=True
)

# Access results
print(f"Motivation: {result.motivation.motivation_score}/100")
print(f"Engagement: {result.engagement.engagement_score}/100")
print(f"Extraversion: {result.big_five.extraversion.score}/100")

# Print summary
pipeline.print_summary(result)
```

## ğŸŒ REST API

```bash
# Start API server
uvicorn api:app --reload

# Assess candidate
curl -X POST http://localhost:8000/assess \
  -F "audio=@interview.wav" \
  -F "candidate_id=C001"
```

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API info |
| `/health` | GET | Health check |
| `/assess` | POST | JSON assessment |
| `/assess/html` | POST | HTML report |

## âš™ï¸ Configuration

### Environment Variables

```bash
# .env
GROQ_API_KEY=gsk_...                       # Required
GROQ_MODEL=llama-3.3-70b-versatile         # Optional
WHISPER_MODEL=base                          # tiny/base/small/medium/large
WHISPER_DEVICE=cuda                         # cpu/cuda
EMOTION_DEVICE=cuda                         # cpu/cuda
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

### Python Configuration

```python
from src.config import PipelineConfig, WhisperConfig, GroqConfig

config = PipelineConfig(
    whisper=WhisperConfig(model_name="medium", device="cuda"),
    groq=GroqConfig(temperature=0.0),  # Deterministic output
)

pipeline = HRAssessmentPipeline(config)
```

## ğŸ› ï¸ Troubleshooting

### CUDA Out of Memory

**Symptoms:**
```
Model detection failed: CUDA out of memory. Tried to allocate 6.19 GiB...
```

**Solutions:**
1. **Use 2 GPUs** (recommended):
   ```bash
   ./run_pipeline.sh  # Auto-selects free GPU
   ```

2. **CPU fallback** (automatic) â€” emotion2vec will run on CPU if GPU OOM occurs

3. **Increase shared memory**:
   ```bash
   --shm-size=16g
   ```

4. **Use CPU mode** (slower):
   ```bash
   -e WHISPER_DEVICE=cpu -e EMOTION_DEVICE=cpu
   ```

### No Free GPUs

```bash
# Check GPU availability
./check_gpu.sh

# Wait for GPUs to free up, or use CPU mode
```

### Inconsistent Motivation Scores

The new deterministic formulas ensure consistency. If you see variations:
- Check that voice features are extracted correctly
- Verify temperature=0.0 in config
- Ensure using latest Docker image

## ğŸ“ Project Structure

```
SA_emotion_detection/
â”œâ”€â”€ main.py                    # CLI entry point
â”œâ”€â”€ api.py                     # FastAPI REST server
â”œâ”€â”€ Dockerfile                 # Docker image
â”œâ”€â”€ docker-compose.yml         # Docker orchestration
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ check_gpu.sh              # GPU availability checker
â”œâ”€â”€ run_pipeline.sh           # Auto GPU selection script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Configuration
â”‚   â”œâ”€â”€ pipeline.py            # Main orchestrator
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py         # Pydantic data models
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ transcription.py   # Whisper speech-to-text
â”‚   â”‚   â”œâ”€â”€ prosody.py         # Pitch, energy, pauses
â”‚   â”‚   â”œâ”€â”€ emotion.py         # emotion2vec detection
â”‚   â”‚   â””â”€â”€ egemaps.py         # eGeMAPS acoustic features
â”‚   â”œâ”€â”€ assessment/
â”‚   â”‚   â”œâ”€â”€ groq_assessor.py   # Groq LLM integration
â”‚   â”‚   â””â”€â”€ prompt_templates.py # Deterministic scoring prompts
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ audio.py           # Audio utilities
â”‚       â””â”€â”€ reporting.py       # HTML report generation
â””â”€â”€ outputs/                   # Generated reports
```

## ğŸ“‹ Requirements

- Python 3.9+
- FFmpeg
- Groq API key â†’ https://console.groq.com
- NVIDIA GPU (optional, for acceleration)
- NVIDIA Container Toolkit (for Docker GPU support)

## ğŸµ Supported Audio Formats

WAV, MP3, M4A, AAC, FLAC, OGG, WebM

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- **Whisper** â€” OpenAI speech-to-text
- **emotion2vec** â€” Alibaba DAMO Academy
- **OpenSMILE** â€” eGeMAPS acoustic features
- **Groq** â€” Fast LLM inference
- **librosa** â€” Audio processing

---

**For detailed GPU setup and troubleshooting, see the helper scripts:**
- `./check_gpu.sh` â€” Check GPU availability
- `./run_pipeline.sh` â€” Auto-select free GPU and run pipeline
